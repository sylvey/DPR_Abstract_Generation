import pandas as pd
from datasets import Dataset
from sklearn.model_selection import train_test_split
import os
import pickle
#!/usr/bin/env python3
from datasets import load_dataset
import torch
from contextlib import nullcontext
from evaluate import load

import shutil
import transformers
from transformers import AutoModelForCausalLM

import numpy as np

from transformers import DataCollatorWithPadding
from contextlib import nullcontext

from transformers import (
        AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig,
        TrainingArguments, Trainer, DataCollatorForLanguageModeling
    )
from peft import LoraConfig, get_peft_model, PeftModel
import math
import random
from datasets import Dataset, DatasetDict

import os
import json
import torch
from safetensors.torch import save_file
from peft import get_peft_model_state_dict
import numbers

def _to_jsonable(x):
    # éè¿´æŠŠç‰©ä»¶è½‰æˆ JSON èƒ½æ¥å—çš„æ±è¥¿
    if x is None or isinstance(x, (bool, str, numbers.Number)):
        return x
    if isinstance(x, (np.integer, np.floating)):
        return x.item()
    if isinstance(x, (list, tuple)):
        return [_to_jsonable(v) for v in x]
    if isinstance(x, set):
        return [_to_jsonable(v) for v in sorted(list(x), key=lambda v: str(v))]
    if isinstance(x, dict):
        return {str(k): _to_jsonable(v) for k, v in x.items()}
    if isinstance(x, torch.dtype):
        return str(x)
    # å…¶ä»–ä¸èªå¾—çš„å‹åˆ¥ â†’ å­—ä¸²ï¼ˆä¿åº•ï¼‰
    return str(x)

def save_lora_adapters_skip_meta(model, out_dir: str, adapter_name: str = "default"):
    os.makedirs(out_dir, exist_ok=True)

    # 1) å– LoRA æ¬Šé‡ï¼Œè·³é meta tensor
    state = get_peft_model_state_dict(model, adapter_name=adapter_name)
    filtered, skipped = {}, []
    for k, v in state.items():
        if getattr(v, "is_meta", False):
            skipped.append(k)
            continue
        filtered[k] = v.detach().to("cpu")
    save_file(filtered, os.path.join(out_dir, "adapter_model.safetensors"))

    # 2) å­˜ adapter configï¼ˆå…ˆåš JSON åŒ–ï¼‰
    cfg = model.peft_config.get(adapter_name, None)
    if cfg is not None:
        cfg_dict = _to_jsonable(cfg.to_dict())
        with open(os.path.join(out_dir, "adapter_config.json"), "w") as f:
            json.dump(cfg_dict, f, indent=2)

    print(f"âœ… Saved LoRA adapter to: {out_dir} (skipped {len(skipped)} meta tensors)")
    if skipped:
        print("   â†³ skipped keys (meta):", skipped[:5], "..." if len(skipped) > 5 else "")

import torch




def main():
    # è¼‰å…¥è³‡æ–™
    global tokenizer, led 
    import os
    os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"


    print("transformers version:", transformers.__version__)
    print("transformers file  :", transformers.__file__)

    # torch.autograd.set_detect_anomaly(True)
    torch.cuda.empty_cache()
    torch.cuda.reset_peak_memory_stats()
    torch.backends.cuda.matmul.allow_tf32 = True

    cache_path = "../data/pubmed_datasets_5000_500_500.pkl"

    if os.path.exists(cache_path):
        print(f"âœ… Loading datasets from cache: {cache_path}")
        with open(cache_path, "rb") as f:
            pubmed_train, pubmed_val, pubmed_test = pickle.load(f)

    else:
        print("ğŸš€ No cache found. Processing from CSV...")
        df = pd.read_csv('../data/all_articles5-v2.csv')

        # rename + clean
        df = df.rename(columns={
            "full_text": "article",
            "abstract": "abstract"
        })
        df = df.dropna(subset=["article", "abstract"])
        df = df[df["article"].str.strip().astype(bool)]
        df = df[df["abstract"].str.strip().astype(bool)]
        df["article"] = df["article"].astype(str)
        df["abstract"] = df["abstract"].astype(str)

        # split
        train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)
        val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)

        pubmed_train = Dataset.from_pandas(train_df[["article", "abstract"]].reset_index(drop=True))
        pubmed_val   = Dataset.from_pandas(val_df[["article", "abstract"]].reset_index(drop=True))
        pubmed_test  = Dataset.from_pandas(test_df[["article", "abstract"]].reset_index(drop=True))

        pubmed_train = pubmed_train.select(range(min(len(pubmed_train), 8000)))
        pubmed_val   = pubmed_val.select(range(min(len(pubmed_val), 800)))
        pubmed_test  = pubmed_test.select(range(min(len(pubmed_test), 800)))

        tokenizer = AutoTokenizer.from_pretrained("./led_pubmed_model")
        encoder_max_length = 512
        decoder_max_length = 512

        def process_data_to_model_inputs(example):
            inputs = tokenizer(
                example["article"],
                padding="max_length",
                truncation=True,
                max_length=encoder_max_length,
            )
            outputs = tokenizer(
                example["abstract"],
                padding="max_length",
                truncation=True,
                max_length=decoder_max_length,
            )
            input_ids = inputs.input_ids
            attention_mask = inputs.attention_mask
            labels = outputs.input_ids

            return {
                "input_ids": input_ids,
                "attention_mask": attention_mask,
                "global_attention_mask": [1] + [0] * (len(input_ids) - 1),
                "labels": [-100 if token == tokenizer.pad_token_id else token for token in labels]
            }

        pubmed_train = pubmed_train.map(
            process_data_to_model_inputs,
            batched=False,
            remove_columns=["article", "abstract"],
            load_from_cache_file=False,
            desc="ğŸ§ª Mapping training data"
        )
        pubmed_val = pubmed_val.map(
            process_data_to_model_inputs,
            batched=False,
            remove_columns=["article", "abstract"],
            load_from_cache_file=False,
            desc="ğŸ§ª Mapping val data"
        )

        # å­˜å¿«å–
        with open(cache_path, "wb") as f:
            pickle.dump((pubmed_train, pubmed_val, pubmed_test), f)
        print(f"ğŸ’¾ Saved processed datasets to {cache_path}")

    print("âœ… pubmed_train size:", len(pubmed_train))
    print("âœ… pubmed_val size:", len(pubmed_val))
    print("âœ… pubmed_test size:", len(pubmed_test))
    print(pubmed_train[0])


    # ---- 0) åŸºæœ¬è¨­å®š ----
    MODEL_ID = "/jet/home/slin23/tmp_ondemand_ocean_cis230089p_symlink/slin23/full_text_label/gpt/gpt-oss-20b"   # TODO: æ›æˆå¯¦éš›æ¨¡å‹ID
    MAX_INPUT_TOKENS = 512
    MAX_TARGET_TOKENS = 64
    MAX_NEW_TOKENS   = 48
    PROMPT_TEMPLATE = (
        "You are a biomedical research assistant.\n"
        "Task: Generate a concise abstract for the following article section.\n"
        "Keep terminology precise; avoid hallucinations; do not fabricate citations.\n"
        "Article:\n{ARTICLE}\n\nWrite the abstract:\n"
    )

    def ensure_article_abstract(ds):
        """è‹¥è³‡æ–™é›†ä¸­æ²’æœ‰ article/abstract æ¬„ä½ï¼Œå›å‚³ None ä»¥è§¸ç™¼é‡å»ºã€‚"""
        f = set(ds.features.keys())
        return ("article" in f and "abstract" in f)

    # ---- 1) æº–å‚™ SFT è³‡æ–™ï¼ˆæŒ‡ä»¤å¼ï¼‰----
    # ä½ çš„ train/val å·²ç¶“è¢« map æˆ LED çš„æ¬„ä½äº†ï¼Œå¯èƒ½ä¸å†æœ‰ article/abstractã€‚
    # è‹¥ç¼ºå°‘ï¼Œé€™è£¡æœƒé‡æ–°å¾ CSV è®€å–ä¸¦ç”¨åŒæ¨£ random_state=42 é‡å»º splitsã€‚
    def rebuild_from_csv_if_needed(train_ds, val_ds, test_ds, csv_path="../data/all_articles5-v2.csv"):
        if ensure_article_abstract(train_ds) and ensure_article_abstract(val_ds) and ensure_article_abstract(test_ds):
            return train_ds, val_ds, test_ds  # ç›´æ¥æ²¿ç”¨

        print("âš ï¸  Detected missing `article/abstract` in train/val; rebuilding splits from CSV for SFT...")
        df_all = pd.read_csv(csv_path)
        df_all = df_all.rename(columns={"full_text": "article", "abstract": "abstract"})
        df_all = df_all.dropna(subset=["article", "abstract"])
        df_all = df_all[df_all["article"].str.strip().astype(bool)]
        df_all = df_all[df_all["abstract"].str.strip().astype(bool)]
        df_all["article"] = df_all["article"].astype(str)
        df_all["abstract"] = df_all["abstract"].astype(str)

        # èˆ‡å‰é¢ä¸€è‡´çš„éš¨æ©Ÿåˆ‡åˆ†
        train_df, temp_df = train_test_split(df_all, test_size=0.2, random_state=42)
        val_df, test_df   = train_test_split(temp_df, test_size=0.5, random_state=42)

        # å°ºåº¦è·Ÿå‰é¢ä¿æŒä¸€è‡´ï¼ˆæœ€å¤š 8000/800/800ï¼‰
        train_df = train_df.iloc[:min(len(train_df), 8000)]
        val_df   = val_df.iloc[:min(len(val_df), 800)]
        test_df  = test_df.iloc[:min(len(test_df), 800)]

        return (
            Dataset.from_pandas(train_df[["article","abstract"]].reset_index(drop=True)),
            Dataset.from_pandas(val_df[["article","abstract"]].reset_index(drop=True)),
            Dataset.from_pandas(test_df[["article","abstract"]].reset_index(drop=True)),
        )

    sft_train_raw, sft_val_raw, sft_test_raw = rebuild_from_csv_if_needed(pubmed_train, pubmed_val, pubmed_test)

    


    # ---- 2) Tokenizer èˆ‡ 4-bit é‡åŒ–æ¨¡å‹ ----
    tok = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)
    tok.pad_token = tok.eos_token if tok.pad_token is None else tok.pad_token
    tok.padding_side = "left"
    tok.truncation_side = "left"   # ç”Ÿæˆæ™‚ä¹Ÿæ›´ç©©

    def _has_min_target_tokens(ex):
        # ç›®æ¨™ï¼šè‡³å°‘ 2 å€‹ tokenï¼ˆå«æˆ‘å€‘å·²ç¶“æœƒåŠ ä¸Šçš„ eosï¼‰
        tgt = tok(ex["abstract"] + tok.eos_token, add_special_tokens=False)["input_ids"]
        return len(tgt) >= 4

    print("ğŸ” filtering samples with too-short targets (<2 tokens after tokenization)â€¦")
    sft_train_raw = sft_train_raw.filter(_has_min_target_tokens)
    sft_val_raw   = sft_val_raw.filter(_has_min_target_tokens)
    sft_test_raw  = sft_test_raw.filter(_has_min_target_tokens)
    print("sizes after filter:", len(sft_train_raw), len(sft_val_raw), len(sft_test_raw))

    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16,  # keep compute in bf16
    )

    
    # ... å‰ç•¥ï¼štok = AutoTokenizer.from_pretrained(MODEL_ID, ...)

    from transformers import AutoModelForCausalLM

    def load_gptoss_mxfaware(model_id: str):
        try:
            # First try: if the model is NOT MXFP4, you can still pass your BnB config here.
            # But because your checkpoint IS MXFP4, this will raise and weâ€™ll fall back.
            return AutoModelForCausalLM.from_pretrained(
                model_id,
                torch_dtype=torch.float16,          # â† æ”¹æˆ fp16
                device_map="auto",
                attn_implementation="eager",
                low_cpu_mem_usage=True,
            )
        except ValueError as e:
            # MXFP4 + BnB mismatch or similar â†’ reload cleanly WITHOUT BnB
            print("ğŸ” Detected MXFP4 or quant-config mismatch; reloading without BitsAndBytesâ€¦")
            return AutoModelForCausalLM.from_pretrained(
                model_id,
                torch_dtype=torch.float16,              # dequantizes MXFP4 to bf16 on V100/Tesla V
                device_map="auto",
                attn_implementation="eager",
                low_cpu_mem_usage=True,
            )

    base_model = load_gptoss_mxfaware(MODEL_ID)

   


    # LoRA è¨­å®šï¼ˆå¯å…ˆå¾ä¿å®ˆå€¼è·‘èµ·ï¼‰
    lora_cfg = LoraConfig(
        r=8, 
        lora_alpha=4, 
        lora_dropout=0.1,
        target_modules=["q_proj","k_proj","v_proj","o_proj"],  # è‹¥åç¨±ä¸åŒï¼Œè«‹æ“šå¯¦èª¿æ•´
        bias="none", task_type="CAUSAL_LM"
    )
    # base_model.gradient_checkpointing_enable()
    model = get_peft_model(base_model, lora_cfg)

    from torch.nn import Parameter, ParameterList

    def _force_paramlist_to_fp16(mod):
        with torch.no_grad():
            # gate_up_proj: ParameterList of weights [E, dim_in, 2*hidden]
            if hasattr(mod, "gate_up_proj") and isinstance(mod.gate_up_proj, ParameterList):
                for i in range(len(mod.gate_up_proj)):
                    p = mod.gate_up_proj[i]
                    if p.device.type != "meta" and p.dtype != torch.float16:
                        mod.gate_up_proj[i] = Parameter(p.to(torch.float16), requires_grad=p.requires_grad)

            # gate_up_proj_bias: ParameterList of biases [E, 2*hidden]
            if hasattr(mod, "gate_up_proj_bias") and isinstance(mod.gate_up_proj_bias, ParameterList):
                for i in range(len(mod.gate_up_proj_bias)):
                    p = mod.gate_up_proj_bias[i]
                    if p.device.type != "meta" and p.dtype != torch.float16:
                        mod.gate_up_proj_bias[i] = Parameter(p.to(torch.float16), requires_grad=p.requires_grad)

            # gate_down_proj: ParameterList of weights [E, 2*hidden, dim_out]
            if hasattr(mod, "gate_down_proj") and isinstance(mod.gate_down_proj, ParameterList):
                for i in range(len(mod.gate_down_proj)):
                    p = mod.gate_down_proj[i]
                    if p.device.type != "meta" and p.dtype != torch.float16:
                        mod.gate_down_proj[i] = Parameter(p.to(torch.float16), requires_grad=p.requires_grad)

    # åªé‡å°å«æœ‰ experts çš„ MLP å®¹å™¨åŸ·è¡Œ
    for name, m in model.named_modules():
        if ("experts" in name) or ("moe" in name):
            _force_paramlist_to_fp16(m)


    def _pre_forward_align_to_fp16(mod, inputs):
        # å°‡ inputsï¼ˆhidden_statesï¼‰å°é½Šåˆ° gate_up_proj çš„ dtypeï¼›åŒæ™‚ç¢ºä¿ ParamList åˆè¢«è½‰å› fp16
        _force_paramlist_to_fp16(mod)

        if not inputs:
            return inputs
        x = inputs[0]
        # ä»£è¡¨æ€§åƒè€ƒ dtypeï¼šè‹¥ gate_up_proj å­˜åœ¨ï¼Œå–ç¬¬ 0 å€‹
        ref_dtype = None
        if hasattr(mod, "gate_up_proj") and isinstance(mod.gate_up_proj, ParameterList) and len(mod.gate_up_proj) > 0:
            ref_dtype = mod.gate_up_proj[0].dtype
        # é æœŸ ref_dtype æœƒæ˜¯ torch.float16ï¼›ä¿éšªå°é½Šä¸€ä¸‹
        if isinstance(x, torch.Tensor) and (ref_dtype is not None) and x.dtype != ref_dtype:
            x = x.to(ref_dtype)
            # å›å‚³æ–°çš„ inputsï¼ˆPyTorch 2 çš„ pre_forward hook æ”¯æ´ä¿®æ”¹è¼¸å…¥ï¼‰
            return (x, ) + tuple(inputs[1:])
        return inputs

    # æ›åœ¨å«æœ‰ experts çš„æ¨¡çµ„ä¸Š
    for name, m in model.named_modules():
        if ("experts" in name) or ("moe" in name):
            try:
                m.register_forward_pre_hook(_pre_forward_align_to_fp16)
            except Exception:
                pass


    left = []
    for n,p in model.named_parameters():
        if (("experts" in n) or ("moe" in n) or ("gate_up_proj" in n) or ("gate_down_proj" in n)) \
        and p.device.type != "meta" and p.is_floating_point() and p.dtype != torch.float16:
            left.append((n, str(p.dtype)))
    print("ğŸ¯ leftover non-fp16 in experts:", left[:10])


   

    # 3) é©—è­‰ï¼ˆå¯ä¿ç•™ï¼›åªåˆ—å‰å¹¾å€‹æ®˜ç•™ BF16 åƒæ•¸åç¨±ï¼‰
    bf16_expert_params = [n for n, p in model.named_parameters()
                        if any(k in n for k in name_hits)
                        and p.is_floating_point() and p.device.type != "meta" and p.dtype == torch.bfloat16]
    if bf16_expert_params:
        print("âš ï¸ Still BF16 expert params (show up to 8):", bf16_expert_params[:8])

    bf16_expert_buffers = [n for n, b in model.named_buffers()
                        if any(k in n for k in name_hits)
                        and torch.is_floating_point(b) and b.device.type != "meta" and b.dtype == torch.bfloat16]
    if bf16_expert_buffers:
        print("âš ï¸ Still BF16 expert buffers (show up to 8):", bf16_expert_buffers[:8])


    # æ‰‹å‹•æŠŠ LoRA B æ¬Šé‡åˆå§‹åŒ–æˆ 0ï¼ˆåˆå§‹ä¸æ”¹è®ŠåŸºç¤æ¨¡å‹ï¼‰
    with torch.no_grad():
        for name, mod in model.named_modules():
            if hasattr(mod, "lora_B"):
                if hasattr(mod.lora_B, "default") and hasattr(mod.lora_B.default, "weight"):
                    mod.lora_B.default.weight.zero_()
                elif hasattr(mod.lora_B, "weight"):
                    mod.lora_B.weight.zero_()

    def print_float_dtypes(tag, model):
        dtypes = set()
        for n, p in model.named_parameters():
            if p.is_floating_point():
                dtypes.add(p.dtype)
        for n, b in model.named_buffers():
            if torch.is_floating_point(b):
                dtypes.add(b.dtype)
        print(f"[{tag}] floating dtypes in model:", dtypes)
    
    
    # print_float_dtypes("before-cast", model)

    # # ğŸ”§ çµ±ä¸€åˆ° float16ï¼ˆHalfï¼‰
    # for n, p in model.named_parameters():
    #     if p.is_floating_point() and p.dtype != torch.float16:
    #         with torch.no_grad():
    #             p.data = p.data.to(torch.float16)
    # for n, b in model.named_buffers():
    #     if torch.is_floating_point(b) and b.dtype != torch.float16:
    #         with torch.no_grad():
    #             b.data = b.data.to(torch.float16)

    # # ä¿éšªï¼šä¹ŸæŠŠ config æ¨™è¨»æˆ fp16
    # setattr(model.config, "torch_dtype", torch.float16)
    # try:
    #     model = model.to(dtype=torch.float16)
    # except Exception:
    #     pass  # device_map=auto æ™‚ï¼Œto() å¯èƒ½ä¸ç§»å‹•åƒæ•¸ï¼Œä½†ä¸å½±éŸ¿æˆ‘å€‘å·²ç¶“é€ä¸€è½‰å‹

    # print_float_dtypes("after-cast", model)

    # åŒæ­¥ pad token dtype ä¸å½±éŸ¿ï¼Œä½†è£œä¸Šé€™è¡Œæ›´ä¸€è‡´
    model.config.pad_token_id = tok.pad_token_id

    # --- MoE routing ç©©å®šåŒ–ï¼ˆæ–°å¢é€™æ®µï¼‰---
    for k, v in [("router_top_k", 1), ("router_jitter_noise", 0.0)]:
        if hasattr(model.config, k):
            setattr(model.config, k, v)


    trainable, total = 0, 0
    for n, p in model.named_parameters():
        total += p.numel()
        if p.requires_grad:
            trainable += p.numel()
    print(f"ğŸ”§ Trainable params: {trainable:,} / {total:,}")
    assert trainable > 0, "âŒ æ²’æœ‰ä»»ä½•å¯è¨“ç·´åƒæ•¸ï¼ˆLoRA target_modules å¯èƒ½å°ä¸åˆ°ï¼‰ã€‚"


    def _stabilize_logits(_, __, out):
        # å…ˆæŠŠ NaN/Inf è½‰æˆæœ‰é™æ•¸ï¼Œå†å¤¾åœ¨ [-50, 50]
        out = torch.nan_to_num(out, nan=0.0, posinf=50.0, neginf=-50.0)
        return out.clamp_(-50, 50)

    # if hasattr(model, "lm_head") and hasattr(model.lm_head, "register_forward_hook"):
    #     model.lm_head.register_forward_hook(_stabilize_logits)

    for m in model.modules():
        # torch.nn.LayerNorm
        if hasattr(m, "eps"):
            m.eps = max(float(m.eps), 1e-5)
        # RMSNorm / è‡ªè¨‚ Norm å¸¸ç”¨çš„æ¬„ä½å
        if hasattr(m, "variance_epsilon"):
            m.variance_epsilon = max(float(m.variance_epsilon), 1e-5)

    model.config.use_cache = False

    print("Sample attn modules:")
    for n,_ in list(model.named_modules())[:300]:
        if "attn" in n.lower() or "attention" in n.lower():
            print(n)
    # âœ… Re-enable gradient checkpointing to cut activations (works with MoE if non-reentrant)
    try:
        model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={'use_reentrant': False})
    except TypeError:
        model.gradient_checkpointing_enable()


    # ---- 3) æŒ‡ä»¤åŒ–æ¨£æœ¬æ˜ å°„ï¼ˆä¸¦åš label maskingï¼šprompt éƒ¨åˆ†ç‚º -100ï¼‰----
    def build_prompt(article: str) -> str:
        return PROMPT_TEMPLATE.replace("{ARTICLE}", article)

    def encode_example(article: str, abstract: str):
        prompt = build_prompt(article)
        prompt_ids  = tok(prompt, add_special_tokens=False)["input_ids"]
        target_ids  = tok(abstract + tok.eos_token, add_special_tokens=False)["input_ids"]

        # ---- å®‰å…¨ä¿åº•ï¼šè‡³å°‘ 2 å€‹ target tokenï¼Œå¦å‰‡ä¸Ÿæ£„è©²æ¨£æœ¬ ----
        if len(target_ids) < 2:
            return {"input_ids": [], "attention_mask": [], "labels": []}  # è®“ map è·³éç©ºæ¨£æœ¬


        target_ids  = target_ids[:MAX_TARGET_TOKENS]
        keep_prompt = max(0, MAX_INPUT_TOKENS - len(target_ids))
        prompt_ids  = prompt_ids[-keep_prompt:] if len(prompt_ids) > keep_prompt else prompt_ids

        input_ids = prompt_ids + target_ids
        labels    = ([-100] * len(prompt_ids)) + target_ids
        attn_mask = [1] * len(input_ids)
        return {
            "input_ids": input_ids,
            "attention_mask": attn_mask,
            "labels": labels,
        }


    def map_sft(ds: Dataset) -> Dataset:
        return ds.map(
            lambda ex: encode_example(ex["article"], ex["abstract"]),
            remove_columns=ds.column_names,
            desc="ğŸ§© Building causal LM samples"
        )
    
    # ==== CACHE: build æŒ‡ä»¤æ¨£æœ¬ï¼ˆmap_sft + filterï¼‰ ====
    def has_label_tokens(ex):
    # åªè¦ labels è£¡æœ‰ä»»ä½•ä¸€å€‹ä¸æ˜¯ -100ï¼Œå°±ä¿ç•™é€™å€‹æ¨£æœ¬
        return any(t != -100 for t in ex["labels"])
    from datasets import load_from_disk

    # ç”¨æ¨¡å‹/Tokenizer & é•·åº¦åƒæ•¸çµ„å‡ºç¨ç‰¹ cache è·¯å¾‘ï¼Œé¿å…æ’æª”
    tok_id = getattr(tok, "name_or_path", "tok")
    SFT_CACHE_DIR = (
        f"../data/sft_cache_"
        f"{os.path.basename(MODEL_ID)}_"
        f"{os.path.basename(str(tok_id))}_"
        f"in{MAX_INPUT_TOKENS}_tgt{MAX_TARGET_TOKENS}_leftpad"
    )

    if os.path.isdir(SFT_CACHE_DIR):
        print(f"âœ… Loading SFT datasets from disk cache: {SFT_CACHE_DIR}")
        sft_train = load_from_disk(os.path.join(SFT_CACHE_DIR, "train"))
        sft_val   = load_from_disk(os.path.join(SFT_CACHE_DIR, "val"))
        sft_test  = load_from_disk(os.path.join(SFT_CACHE_DIR, "test"))
    else:
        print("ğŸ§© Building causal LM samples (first time)...")
        sft_train = map_sft(sft_train_raw).filter(has_label_tokens)
        sft_val   = map_sft(sft_val_raw).filter(has_label_tokens)
        sft_test  = map_sft(sft_test_raw)  # æ¸¬è©¦é›†å¯ä¸éæ¿¾ label

        print(f"ğŸ’¾ Saving SFT datasets to: {SFT_CACHE_DIR}")
        os.makedirs(SFT_CACHE_DIR, exist_ok=True)
        sft_train.save_to_disk(os.path.join(SFT_CACHE_DIR, "train"))
        sft_val.save_to_disk(os.path.join(SFT_CACHE_DIR, "val"))
        sft_test.save_to_disk(os.path.join(SFT_CACHE_DIR, "test"))

    print("âœ… SFT sizes:", len(sft_train), len(sft_val), len(sft_test))
    # ==== END CACHE ====

    

    def left_pad_collator(features):
    # å– batch å…§çš„æœ€é•·åºåˆ—é•·åº¦
        max_len = max(len(f["input_ids"]) for f in features)

        input_ids, attention_mask, labels = [], [], []
        for f in features:
            ids  = f["input_ids"]
            mask = f["attention_mask"]
            lab  = f["labels"]

            pad = max_len - len(ids)
            # å·¦å´è£œé½Š
            input_ids.append([tok.pad_token_id]*pad + ids)
            attention_mask.append([0]*pad + mask)
            labels.append([-100]*pad + lab)

        return {
            "input_ids": torch.tensor(input_ids, dtype=torch.long),
            "attention_mask": torch.tensor(attention_mask, dtype=torch.long),
            "labels": torch.tensor(labels, dtype=torch.long),
        }
    
    collator = left_pad_collator

    # ---- 5) è¨“ç·´åƒæ•¸ ----
    class NoMoveTrainer(Trainer):
        def __init__(self, *args, **kwargs):
            super().__init__(*args, **kwargs)
            self.skipped_batches = 0

        def _move_model_to_device(self, model, device):
            # æˆ‘å€‘è‡ªå·±ç®¡ç† device_mapï¼ˆHF auto/shardï¼‰ï¼Œæ‰€ä»¥ä¸è¦ Trainer å†æ¬å‹•
            return model

        @staticmethod
        def _safe_zero(model):
            # å›å‚³ã€Œæ›åœ¨åƒæ•¸åœ–ä¸Šçš„ 0ã€ï¼Œè®“ backward æœ‰ grad_fn ä½†å¯¦éš›ä¸æ›´æ–°
            for p in model.parameters():
                if p.requires_grad:
                    return p.sum() * 0.0
            # è¬ä¸€æ²’æœ‰å¯è¨“ç·´åƒæ•¸ï¼ˆç†è«–ä¸Šä¸æœƒï¼‰ï¼Œä»å›ä¸€å€‹éœ€è¦æ¢¯åº¦çš„ 0
            dev = next(model.parameters()).device
            return torch.tensor(0.0, device=dev, requires_grad=True)

        def compute_loss(self, model, inputs, num_items_in_batch=None, **kwargs):
            # â‘  å…ˆå¿«é€Ÿæª¢æŸ¥ï¼šæ˜¯å¦é€™å€‹ batch æ ¹æœ¬æ²’æœ‰å¯å­¸çš„ label
            labels = inputs.get("labels", None)
            if labels is not None and (labels != -100).sum() == 0:
                self.skipped_batches += 1
                print("âš ï¸ batch has 0 valid labels; skipping.")
                # æ¸…æ‰æ®˜ç•™ gradï¼ˆä¿éšªï¼‰
                for p in model.parameters():
                    if p.grad is not None:
                        p.grad = None
                return self._safe_zero(model)

            # â‘¡ æ­£å¸¸ forward
            outputs = model(**inputs)
            loss = outputs["loss"] if isinstance(outputs, dict) else getattr(outputs, "loss", None)

            # â‘¢ è¦ç¯„æˆ scalar tensor
            if loss is not None:
                loss = loss.mean()

            # â‘£ æª¢æŸ¥ NaN/Inf æˆ–ç„¡æ¢¯åº¦ï¼ˆä¾‹å¦‚æŸäº›å¯¦ä½œå›å‚³å¸¸æ•¸ 0ï¼‰
            bad = (loss is None) or (not torch.isfinite(loss)) or (not loss.requires_grad)
            if bad:
                self.skipped_batches += 1
                if loss is None:
                    print("âš ï¸ loss=None; skipping this batch.")
                elif not torch.isfinite(loss):
                    print("âš ï¸ non-finite loss detected; skipping this batch.")
                elif not loss.requires_grad:
                    print("âš ï¸ loss has no grad; skipping this batch.")
                # # æ¸…æ‰æ®˜ç•™ gradï¼Œé¿å…æ±¡æŸ“
                # for p in model.parameters():
                #     if p.grad is not None:
                #         p.grad = None
                return self._safe_zero(model)

            # â‘¤ æ­£å¸¸å›å‚³ lossï¼ˆå¯è¢« AMP/Accelerate scale/backwardï¼‰
            return loss




    from inspect import signature

    def make_training_args(**common):
        sig = signature(TrainingArguments.__init__).parameters
        # éæ¿¾æ‰ä¸æ”¯æ´çš„åƒæ•¸ï¼ˆé¿å…è€ç‰ˆ transformers çˆ†ï¼‰
        common = {k: v for k, v in common.items() if k in sig}

        if "evaluation_strategy" in sig:
            common["evaluation_strategy"] = "no"   # â† é—œæ‰è¨“ç·´ä¸­çš„ eval
        elif "eval_strategy" in sig:
            common["eval_strategy"] = "no"
        elif "do_eval" in sig:
            common["do_eval"] = False

        # ğŸ”’ é—œä¿å­˜ç­–ç•¥ï¼Œé¿å… Trainer è‡ªå·±å­˜
        if "save_strategy" in sig:
            common["save_strategy"] = "no"
        # ä¸éœ€è¦ save_steps
        common.pop("save_steps", None)

        return TrainingArguments(**common)

    # å…±åŒåƒæ•¸ï¼ˆä¿æŒä½ åŸæœ¬è¨­å®šï¼‰
    common_args = dict(
        output_dir="gptoss20b_lora_abs",
        per_device_train_batch_size=1,
        per_device_eval_batch_size=1,
        gradient_accumulation_steps=16,
        max_steps=50, 
        learning_rate=2e-6,
        num_train_epochs=2,
        max_grad_norm=0.5,
        logging_steps=5,
        # save_steps=1000,
        weight_decay=0.01,
        warmup_ratio=0.10,
        lr_scheduler_type="cosine",
        bf16=False,   # V100
        fp16=True,    # V100 é–‹ FP16
        gradient_checkpointing=True,      
        # evaluation_strategy="no",         # æ²’æœ‰ evaluation strategy åƒæ•¸
        # predict_with_generate=False, # æ²’æœ‰ predict_with_generate åƒæ•¸
        report_to="none",
        remove_unused_columns=False,
    )



    args = make_training_args(**common_args)
    print("âœ… TrainingArguments constructed with:", args)
    

    # after args is constructed
    amp_dtype = torch.float16 if getattr(args, "fp16", False) else (torch.bfloat16 if getattr(args, "bf16", False) else None)
    amp_ctx = (torch.cuda.amp.autocast(dtype=amp_dtype) if amp_dtype is not None else nullcontext())

    from contextlib import contextmanager

    @contextmanager
    def temporarily_enable_cache(model):
        prev = bool(getattr(model.config, "use_cache", False))
        model.config.use_cache = True
        try:
            yield
        finally:
            model.config.use_cache = prev


    dl = torch.utils.data.DataLoader(sft_train, batch_size=1, collate_fn=collator)

    chk = torch.utils.data.DataLoader(sft_train, batch_size=1, collate_fn=collator)
    for i, b in enumerate(chk):
        nvalid = (b["labels"] != -100).sum().item()
        if nvalid == 0:
            print(f"â— batch {i} has 0 valid labels")
            break
        if i > 50:  # çœ‹å‰ 50 å€‹å°±å¥½
            break


    batch = next(iter(dl))

    def find_first_nan_module(model, batch, amp_ctx):
        bad = {"name": None}
        handles = []

        def make_hook(name):
            def hook(_, __, out):
                t = out[0] if isinstance(out, tuple) else out
                if isinstance(t, torch.Tensor) and not torch.isfinite(t).all():
                    bad["name"] = name
                    raise RuntimeError(f"NaN in {name}")
            return hook

        for name, m in model.named_modules():
            if any(k in name.lower() for k in ["layer","block","mlp","experts","attention","router","norm"]):
                handles.append(m.register_forward_hook(make_hook(name)))

        try:
            with torch.no_grad(), amp_ctx:
                _ = model(
                    input_ids=batch["input_ids"].to(model.device),
                    attention_mask=batch["attention_mask"].to(model.device),
                )
        except Exception as e:
            print("First NaN at module:", bad["name"], "|", e)
        finally:
            for h in handles:
                h.remove()

    # å‘¼å«
    find_first_nan_module(model, batch, amp_ctx)


    num_valid = (batch["labels"] != -100).sum().item()
    print("valid tokens in labels:", num_valid)
    assert num_valid > 0, "All labels are -100; loss would be undefined."

    # å¯é¸ï¼šé–‹ hidden states ä¾†çœ‹æ˜¯å¦å‰é¢å°± NaN äº†
    with torch.no_grad(), amp_ctx:
        # with amp_ctx:
        out_dbg = model(
            input_ids=batch["input_ids"].to(model.device),
            attention_mask=batch["attention_mask"].to(model.device),
            labels=None,
            output_hidden_states=True,
            return_dict=True,
        )
    hs = out_dbg.hidden_states[-1]
    print("last hidden finite?", torch.isfinite(hs).all().item())


    # ---- 6) ROUGE è©•ä¼°ï¼ˆç”¨ç”Ÿæˆï¼‰----
    rouge = load("rouge")

    def generate_text(batch):
        outs = []
        with temporarily_enable_cache(model):               # â† é€™è¡Œ
            for art in batch["article"]:
                prompt = build_prompt(art)
                ids = tok(prompt, return_tensors="pt", truncation=True, max_length=MAX_INPUT_TOKENS).to(model.device)
                with torch.no_grad(), amp_ctx:              # â† å’Œè¨“ç·´ä¸€è‡´çš„ AMP
                    gen = model.generate(
                        **ids,
                        max_new_tokens=MAX_NEW_TOKENS,
                        temperature=0.0,
                        top_p=0.0,
                        do_sample=False,
                        eos_token_id=tok.eos_token_id,
                    )
                text = tok.decode(gen[0], skip_special_tokens=True)
                outs.append(text[len(prompt):].strip())
        return outs


    def compute_metrics_eval(eval_pred):
        sample_n = min(200, len(sft_val_raw))
        articles = [sft_val_raw[i]["article"] for i in range(sample_n)]
        refs     = [sft_val_raw[i]["abstract"] for i in range(sample_n)]

        preds = []
        with temporarily_enable_cache(model):               
            for art in articles:
                prompt = build_prompt(art)
                ids = tok(prompt, return_tensors="pt", truncation=True, max_length=MAX_INPUT_TOKENS).to(model.device)
                with torch.no_grad(), amp_ctx:              
                    gen = model.generate(
                        **ids,
                        max_new_tokens=MAX_NEW_TOKENS,
                        temperature=0.0,
                        top_p=1.0,
                        do_sample=False,
                        eos_token_id=tok.eos_token_id,
                    )
                text = tok.decode(gen[0], skip_special_tokens=True)
                preds.append(text[len(prompt):].strip())

        scores = rouge.compute(predictions=preds, references=refs, use_stemmer=True)
        scores["gen_len"] = sum(len(p.split()) for p in preds) / max(1, len(preds))
        return scores


    

    # try:
    #     model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={'use_reentrant': False})
    # except TypeError:
    #     model.gradient_checkpointing_enable()

    # 4) ğŸ›¡ï¸ ç¢ºä¿ router æ²’éš¨æ©Ÿæ€§ï¼ˆè‹¥é€™äº›å±¬æ€§å­˜åœ¨å°±è¨­ 0ï¼›ä¸å­˜åœ¨å°±å¿½ç•¥ï¼‰
    for attr in ["router_dropout", "expert_dropout", "hidden_dropout", "attention_dropout", "embd_pdrop"]:
        if hasattr(model.config, attr):
            setattr(model.config, attr, 0.0)

    


    trainer = NoMoveTrainer(
        model=model,
        args=args,
        train_dataset=sft_train,
        eval_dataset=sft_val,
        data_collator=collator,
        tokenizer=tok,   
        # processing_class=tok,      # â† å–ä»£ tokenizer=tok
        compute_metrics=compute_metrics_eval,
    )
    from torch.utils.data import DataLoader
    model.train()
    opt = torch.optim.AdamW([p for p in model.parameters() if p.requires_grad], lr=1e-4)

    test_loader = DataLoader(sft_train, batch_size=1, collate_fn=collator)
    batch = next(iter(test_loader))
    batch = {k: v.to(model.device) for k, v in batch.items()}

    with torch.no_grad():
        out = model(**batch)
    print("quick sanity loss:", float(out.loss))
    # ä¸è¦æ‰‹å‹• backwardï¼Œäº¤çµ¦ Trainer



    print("ğŸš€ Start training gpt-oss-20b LoRAâ€¦")
    trainer.train()
    print("Skipped batches:", trainer.skipped_batches)


    save_dir = "gptoss20b_lora_abs"
    save_lora_adapters_skip_meta(model, save_dir)   
    # trainer.save_model("gptoss20b_lora_abs")


    @torch.no_grad()
    def batched_generate_preds_refs(ds, batch_size=2, max_items=None):
        N = len(ds) if max_items is None else min(len(ds), max_items)
        preds, refs = [], []
        with temporarily_enable_cache(model):               # â† é€™è¡Œ
            for start in range(0, N, batch_size):
                end = min(start + batch_size, N)
                articles   = [ds[i]["article"] for i in range(start, end)]
                references = [ds[i]["abstract"] for i in range(start, end)]

                prompts = [build_prompt(a) for a in articles]
                inputs = tok(
                    prompts,
                    return_tensors="pt",
                    padding=True,
                    truncation=True,
                    max_length=MAX_INPUT_TOKENS,
                ).to(model.device)

                with amp_ctx:                               # â† AMP
                    outputs = model.generate(
                        **inputs,
                        max_new_tokens=MAX_NEW_TOKENS,
                        do_sample=False,
                        temperature=0.0,
                        top_p=1.0,
                        eos_token_id=tok.eos_token_id,
                    )
                texts = tok.batch_decode(outputs, skip_special_tokens=True)

                for full, prompt, ref in zip(texts, prompts, references):
                    preds.append(full[len(prompt):].strip())
                    refs.append(ref)
        return preds, refs



    print("ğŸ“ Evaluating on pubmed_val (ROUGE-1/2/L)â€¦")
    # å¦‚éœ€åŠ å¿«é¦–æ¬¡è·‘æ¸¬å¯åŠ  max_items=200ï¼›æ­£å¼è©•ä¼°æ‹¿æ‰å³å¯
    predictions, references = batched_generate_preds_refs(sft_val_raw, batch_size=2)

    rouge_scores = rouge.compute(
        predictions=predictions,
        references=references,
        use_stemmer=True
    )
    # evaluate çš„ ROUGE æœƒå›å‚³ 0~1 çš„åˆ†æ•¸
    print(
        "ROUGE-1: {:.4f} | ROUGE-2: {:.4f} | ROUGE-L: {:.4f}".format(
            rouge_scores["rouge1"], rouge_scores["rouge2"], rouge_scores["rougeL"]
        )
    )



if __name__ == "__main__":
    main()